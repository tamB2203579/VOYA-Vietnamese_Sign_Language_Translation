{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c185b6f9",
   "metadata": {},
   "source": [
    "**IMPORT DEPENDENCIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f206d4ae",
   "metadata": {},
   "source": [
    "**DEFINE KEY FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ba9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mediapipe hands model\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "def extract_keypoints(image, results):\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        return np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]) if hand_landmarks.landmark else np.zeros((21,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mediapipe holistic model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             )\n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # Draw right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308d5c2",
   "metadata": {},
   "source": [
    "**TEST STATIC MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "DATA_PATH = \"./data/alphabets\"\n",
    "\n",
    "# Load trained model\n",
    "model = load_model(\"models/alphabets.keras\")\n",
    "threshold = 0.8\n",
    "\n",
    "# Class labels\n",
    "all_folders = os.listdir(DATA_PATH)\n",
    "alphabets = sorted(set([f.split(\"_\")[0] for f in all_folders]))\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        image, results = mediapipe_detection(frame, hands)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            draw_landmarks(image, results)\n",
    "            keypoints = extract_keypoints(image, results)\n",
    "            if keypoints is not None and keypoints.size > 0:\n",
    "                prediction = model.predict(np.expand_dims(keypoints, axis=0), verbose=0)\n",
    "                class_id = np.argmax(prediction)\n",
    "                confidence = np.max(prediction)\n",
    "\n",
    "                if confidence > threshold:\n",
    "                    label = alphabets[class_id]\n",
    "                    cv2.putText(image, f'{label} {int(confidence * 100)}%', (10, 70),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 3)\n",
    "\n",
    "        cv2.imshow('VSL Alphabets Recognition', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455a06b5",
   "metadata": {},
   "source": [
    "**TEST DYNAMIC MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "DATA_PATH = \"./data/alphabets\"\n",
    "\n",
    "# Load trained model\n",
    "model = load_model(\"models/alphabets.keras\")\n",
    "threshold = 0.8\n",
    "\n",
    "# Class labels\n",
    "all_folders = os.listdir(DATA_PATH)\n",
    "actions = sorted(set([f.split(\"_\")[0] for f in all_folders]))\n",
    "# actions = np.array(os.listdir(DATA_PATH))\n",
    "\n",
    "# Frame buffer\n",
    "sequence = []\n",
    "SEQ_LENGTH = 30\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "with mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    waiting = True\n",
    "    start_recording = False\n",
    "    countdown = 3\n",
    "    countdown_start_time = None\n",
    "    show_waiting_status = True\n",
    "\n",
    "    last_label = None\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        image, results = mediapipe_detection(frame, hands)\n",
    "        key = cv2.waitKey(10) & 0xFF\n",
    "\n",
    "        # -------- WAITING STATE --------\n",
    "        if waiting:\n",
    "            if show_waiting_status:\n",
    "                cv2.putText(image, \"Waiting... (press 's')\", (20, 40),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "                \n",
    "            if last_label is not None:\n",
    "                cv2.putText(image, last_label, (20, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 3)\n",
    "                \n",
    "            if key == ord(\"s\"):\n",
    "                waiting = False\n",
    "                show_waiting_status = False\n",
    "                sequence = []\n",
    "                start_recording = True\n",
    "                countdown_start_time = cv2.getTickCount()\n",
    "                last_label = None\n",
    "\n",
    "        # -------- COUNTDOWN + CAPTURE STATE --------\n",
    "        elif start_recording:\n",
    "            # ensure countdown timer exists\n",
    "            if countdown_start_time is None:\n",
    "                countdown_start_time = cv2.getTickCount()\n",
    "\n",
    "            elapsed_time = (cv2.getTickCount() - countdown_start_time) / cv2.getTickFrequency()\n",
    "\n",
    "            if elapsed_time < countdown:\n",
    "                number = countdown - int(elapsed_time)\n",
    "                cv2.putText(image, f\"{number}\", (20, 40),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
    "            else:\n",
    "                if results.multi_hand_landmarks:\n",
    "                    draw_landmarks(image, results)\n",
    "                    keypoints = extract_keypoints(image, results)\n",
    "\n",
    "                    if keypoints is not None and keypoints.size > 0:\n",
    "                        keypoints = keypoints.flatten()\n",
    "                        sequence.append(keypoints)\n",
    "\n",
    "                        cv2.putText(image, f\"Capturing... {len(sequence)}/{SEQ_LENGTH}\", (20, 40),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "                        if len(sequence) == SEQ_LENGTH:\n",
    "                            # Predict\n",
    "                            res = model.predict(np.expand_dims(sequence, axis=0), verbose=0)\n",
    "                            class_id = int(np.argmax(res))\n",
    "                            confidence = float(np.max(res))\n",
    "\n",
    "                            if confidence > threshold:\n",
    "                                label = actions[class_id]\n",
    "                                cv2.putText(image, f'{label} {int(confidence * 100)}%',\n",
    "                                            (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 3)\n",
    "                            else:\n",
    "                                cv2.putText(image, \"No confident enough to guess\",\n",
    "                                            (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "\n",
    "                            # Reset to waiting\n",
    "                            waiting = True\n",
    "                            start_recording = False\n",
    "                            countdown_start_time = None\n",
    "                            sequence = []\n",
    "\n",
    "        cv2.imshow('VSL Recognition', image)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
