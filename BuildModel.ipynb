{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c185b6f9",
   "metadata": {},
   "source": [
    "**IMPORT DEPENDENCIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dac8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv1D, MaxPooling1D\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential\n",
    "\n",
    "IMAGE_PATH = os.path.join(\"Images\")\n",
    "DATA_PATH = os.path.join(\"Data/static\")\n",
    "SAVE_DIR = os.path.join(\"result/static\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f206d4ae",
   "metadata": {},
   "source": [
    "**DEFINE KEY FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ba9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "    \n",
    "def extract_keypoints(image, results):\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        return np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]) if hand_landmarks.landmark else np.zeros((21,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a03bf",
   "metadata": {},
   "source": [
    "**EXTRACT KEYPOINTS USING MEDIAPIPE HANDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156171f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.7, max_num_hands=1) as hands:\n",
    "    path = os.path.join(IMAGE_PATH)\n",
    "    for dir in os.listdir(path):\n",
    "        dir_path = os.path.join(path, dir)\n",
    "        for file in tqdm(os.listdir(dir_path)):\n",
    "            \n",
    "            image_path = os.path.join(dir_path, file)\n",
    "            image = cv2.imread(image_path)\n",
    "\n",
    "            if image is None:\n",
    "                continue\n",
    "\n",
    "            image, results = mediapipe_detection(image, hands)\n",
    "            keypoints = extract_keypoints(image, results)\n",
    "            \n",
    "            if keypoints:\n",
    "                os.makedirs(os.path.join(DATA_PATH, dir), exist_ok=True)\n",
    "                save_path = os.path.join(DATA_PATH, dir, file.rsplit(\".\",1)[0] + \".npy\")\n",
    "                np.save(save_path, keypoints)\n",
    "\n",
    "        print(\"Processed directory:\", dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603fcb66",
   "metadata": {},
   "source": [
    "**LABEL DATA FOR TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5805c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 11/24 [00:22<00:30,  2.34s/it]"
     ]
    }
   ],
   "source": [
    "alphabets = np.array(os.listdir(DATA_PATH))\n",
    "label_map = {label: num for num, label in enumerate(alphabets)}\n",
    "\n",
    "data, labels = [], []\n",
    "  \n",
    "for label in tqdm(alphabets):\n",
    "    path = os.path.join(DATA_PATH, label, \"npy\")\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        try:\n",
    "            keypoints = np.load(file_path)\n",
    "            data.append(keypoints)\n",
    "            labels.append(label_map[label])\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping file {file_path} due to error: {e}\")\n",
    "\n",
    "# Convert to arrays and one-hot encode labels\n",
    "X = np.array(data)\n",
    "y = to_categorical(np.array(labels), num_classes=len(alphabets))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6650bcfa",
   "metadata": {},
   "source": [
    "**SAVE TRAIN DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "np.save(f\"{SAVE_DIR}/X_train.npy\", X_train)\n",
    "np.save(f\"{SAVE_DIR}/X_test.npy\", X_test)\n",
    "np.save(f\"{SAVE_DIR}/y_train.npy\", y_train)\n",
    "np.save(f\"{SAVE_DIR}/y_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88580cf7",
   "metadata": {},
   "source": [
    "**LOAD TRAIN DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a248689",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(f\"{SAVE_DIR}/X_train.npy\")\n",
    "X_test = np.load(f\"{SAVE_DIR}/X_test.npy\")\n",
    "y_train = np.load(f\"{SAVE_DIR}/y_train.npy\")\n",
    "y_test = np.load(f\"{SAVE_DIR}/y_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ace35df",
   "metadata": {},
   "source": [
    "**TRAIN MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7448570",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(\"logs\")\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "model = Sequential([\n",
    "    Input(shape=(21, 3)), # 21 keypoints of hand x 3 coordinates (x,y,z)\n",
    "    Conv1D(64, 3, activation='relu'),\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(128, 3, activation='relu'),\n",
    "    MaxPooling1D(2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(12, activation='softmax') # Change the number to match the number of class\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f6aac",
   "metadata": {},
   "source": [
    "**BUILD MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8137c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=30, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e9dd8",
   "metadata": {},
   "source": [
    "**SAVE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e5373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/static.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308d5c2",
   "metadata": {},
   "source": [
    "**TEST STATIC MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"models/static.h5\")\n",
    "threshold = 0.8\n",
    "\n",
    "alphabets = np.array(os.listdir(DATA_PATH))\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        image, results = mediapipe_detection(frame, hands)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            draw_landmarks(image, results)\n",
    "            keypoints = extract_keypoints(image, results)\n",
    "            if keypoints is not None and keypoints.size > 0:\n",
    "                keypoints = keypoints.reshape(21, 3)\n",
    "                prediction = model.predict(np.expand_dims(keypoints, axis=0), verbose=0)\n",
    "                class_id = np.argmax(prediction)\n",
    "                confidence = np.max(prediction)\n",
    "\n",
    "                if confidence > threshold:\n",
    "                    label = alphabets[class_id]\n",
    "                    cv2.putText(image, f'{label} {int(confidence * 100)}%', (10, 70),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 3)\n",
    "\n",
    "        cv2.imshow('ASL Recognition', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455a06b5",
   "metadata": {},
   "source": [
    "**TEST DYNAMIC MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "DATA_PATH = \"./Data/dynamic\"\n",
    "\n",
    "# Load trained model\n",
    "model = load_model(\"models/dynamic.h5\")\n",
    "threshold = 0.8\n",
    "\n",
    "# Class labels\n",
    "actions = np.array(os.listdir(DATA_PATH))\n",
    "\n",
    "# Frame buffer\n",
    "sequence = []\n",
    "SEQ_LENGTH = 30\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    start_recording = False\n",
    "    countdown = 3\n",
    "    countdown_start_time = None\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        image, results = mediapipe_detection(frame, hands)\n",
    "\n",
    "        if not start_recording:\n",
    "            if countdown_start_time is None:\n",
    "                countdown_start_time = cv2.getTickCount()\n",
    "\n",
    "            elapsed_time = (cv2.getTickCount() - countdown_start_time) / cv2.getTickFrequency()\n",
    "\n",
    "            # Hiển thị số đếm ngược\n",
    "            if elapsed_time < countdown:\n",
    "                number = countdown - int(elapsed_time)\n",
    "                cv2.putText(image, f\"{number}\", (250, 200),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 0, 255), 6)\n",
    "            else:\n",
    "                start_recording = True  # hết countdown thì bắt đầu\n",
    "                sequence = [] \n",
    "        else:\n",
    "            # Khi đã start thì xử lý Mediapipe keypoints\n",
    "            if results.multi_hand_landmarks:\n",
    "                draw_landmarks(image, results)\n",
    "                keypoints = extract_keypoints(image, results)\n",
    "\n",
    "                if keypoints is not None and keypoints.size > 0:\n",
    "                    keypoints = keypoints.flatten()\n",
    "                    sequence.append(keypoints)\n",
    "\n",
    "                    if len(sequence) > SEQ_LENGTH:\n",
    "                        sequence.pop(0)\n",
    "\n",
    "                    # Hiển thị visual cue: khung xanh + chữ \"Recording...\"\n",
    "                    h, w, _ = image.shape\n",
    "                    cv2.rectangle(image, (10, 10), (w-10, h-10), (0, 255, 0), 6)\n",
    "                    cv2.putText(image, \"Recording...\", (50, 60),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)\n",
    "\n",
    "                    if len(sequence) == SEQ_LENGTH:\n",
    "                        input_data = np.expand_dims(sequence, axis=0)  # (1,30,63)\n",
    "                        prediction = model.predict(input_data, verbose=0)\n",
    "                        class_id = np.argmax(prediction)\n",
    "                        confidence = np.max(prediction)\n",
    "\n",
    "                        if confidence > threshold:\n",
    "                            label = actions[class_id]\n",
    "                            cv2.putText(image, f'{label} {int(confidence * 100)}%',\n",
    "                                        (10, 120), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                        2, (255, 0, 0), 3)\n",
    "\n",
    "        cv2.imshow('ASL Recognition', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
