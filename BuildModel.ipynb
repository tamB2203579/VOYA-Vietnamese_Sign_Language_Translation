{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c185b6f9",
   "metadata": {},
   "source": [
    "**IMPORT DEPENDENCIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPooling1D\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import load_model\n",
    "\n",
    "IMAGE_PATH = os.path.join(\"Images\")\n",
    "DATA_PATH = os.path.join(\"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f206d4ae",
   "metadata": {},
   "source": [
    "**DEFINE KEY FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ba9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "    \n",
    "def extract_keypoints(image, results):\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        keypoints = []\n",
    "        for landmrk in hand_landmarks.landmark:\n",
    "            keypoints.append({\"x\": landmrk.x, \"y\": landmrk.y, \"z\": landmrk.z})\n",
    "        return keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a03bf",
   "metadata": {},
   "source": [
    "**EXTRACT KEYPOINTS USING MEDIAPIPE HANDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156171f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.7, max_num_hands=1) as hands:\n",
    "    path = os.path.join(IMAGE_PATH, \"normal\")\n",
    "    for dir in os.listdir(path)[:19]:\n",
    "        dir_path = os.path.join(path, dir)\n",
    "        for file in tqdm(os.listdir(dir_path)):\n",
    "            \n",
    "            image_path = os.path.join(dir_path, file)\n",
    "            image = cv2.imread(image_path)\n",
    "\n",
    "            if image is None:\n",
    "                continue\n",
    "\n",
    "            image, results = mediapipe_detection(image, hands)\n",
    "            \n",
    "            keypoints = extract_keypoints(image, results)\n",
    "            \n",
    "            if keypoints:\n",
    "                if not os.path.exists(os.path.join(DATA_PATH, dir)):\n",
    "                    os.makedirs(os.path.join(DATA_PATH, dir))\n",
    "                with open(os.path.join(DATA_PATH, dir, file.replace(\".jpg\", \".json\")), \"w\") as f:\n",
    "                    json.dump(keypoints, f)\n",
    "\n",
    "        print(\"Processed directory:\", dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603fcb66",
   "metadata": {},
   "source": [
    "**LABEL DATA FOR TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5805c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = np.array(os.listdir(DATA_PATH))\n",
    "label_map = {label: num for num, label in enumerate(alphabets)}\n",
    "\n",
    "data, labels = [], []\n",
    "  \n",
    "for label in tqdm(alphabets):\n",
    "    path = os.path.join(DATA_PATH, label)\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                keypoints = json.load(f)\n",
    "\n",
    "            flattened_keypoints = np.array([[landmrk[\"x\"], landmrk[\"y\"], landmrk[\"z\"]] for landmrk in keypoints]).flatten()\n",
    "\n",
    "            data.append(flattened_keypoints)\n",
    "            labels.append(label_map[label])\n",
    "\n",
    "        except (ValueError, TypeError, json.JSONDecodeError) as e:\n",
    "            print(f\"Skipping file {file_path} due to error: {e}\")\n",
    "\n",
    "# Convert to arrays and one-hot encode labels\n",
    "X = np.array(data)\n",
    "y = to_categorical(np.array(labels), num_classes=len(alphabets))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6650bcfa",
   "metadata": {},
   "source": [
    "**SAVE TRAIN DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"y_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88580cf7",
   "metadata": {},
   "source": [
    "**LOAD TRAIN DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a248689",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = np.load(\"Result/X_train.npy\"), np.load(\"Result/X_test.npy\"), np.load(\"Result/y_train.npy\"), np.load(\"Result/y_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ace35df",
   "metadata": {},
   "source": [
    "**TRAIN MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7448570",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(\"Logs\")\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model = Sequential()\n",
    "# The first convolutional layer\n",
    "model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# The second convolutional layer\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(23, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f6aac",
   "metadata": {},
   "source": [
    "**BUILD MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8137c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=15, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e9dd8",
   "metadata": {},
   "source": [
    "**SAVE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e5373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Models/action.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308d5c2",
   "metadata": {},
   "source": [
    "**LOAD MODEL AND TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"Models/action.h5\")\n",
    "threshold = 0.8\n",
    "\n",
    "alphabets = np.array(os.listdir(DATA_PATH))\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        image, results = mediapipe_detection(frame, hands)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            draw_landmarks(image, results)\n",
    "        \n",
    "            keypoints = extract_keypoints(image, results)\n",
    "            \n",
    "            if keypoints:\n",
    "                # Convert keypoints from list of dictionaries to flattened array\n",
    "                flattened_keypoints = np.array([[landmrk[\"x\"], landmrk[\"y\"], landmrk[\"z\"]] for landmrk in keypoints]).flatten()\n",
    "                \n",
    "                prediction = model.predict(np.expand_dims(flattened_keypoints, axis=0), verbose=0)\n",
    "                class_id = np.argmax(prediction)\n",
    "                confidence = np.max(prediction)\n",
    "\n",
    "                if confidence > threshold:\n",
    "                    label = alphabets[class_id]\n",
    "                    cv2.putText(image, f'{label} {int(confidence * 100)}%', (10, 70),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 3)\n",
    "\n",
    "        cv2.imshow('ASL Recognition', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
