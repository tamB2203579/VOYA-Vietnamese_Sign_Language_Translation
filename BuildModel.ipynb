{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c185b6f9",
   "metadata": {},
   "source": [
    "**IMPORT DEPENDENCIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.api.utils import to_categorical\n",
    "from keras.api.callbacks import TensorBoard\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n",
    "from keras.api.models import load_model\n",
    "\n",
    "IMAGE_PATH = os.path.join(\"Images\")\n",
    "DATA_PATH = os.path.join(\"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f206d4ae",
   "metadata": {},
   "source": [
    "**DEFINE KEY FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ba9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "    \n",
    "def extract_keypoints(image, results):\n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        keypoints = []\n",
    "        for landmrk in hand_landmarks.landmark:\n",
    "            keypoints.append({\"x\": landmrk.x, \"y\": landmrk.y, \"z\": landmrk.z})\n",
    "        return keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a03bf",
   "metadata": {},
   "source": [
    "**EXTRACT KEYPOINTS ON KAGGLE ASL IMAGE DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156171f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.7, max_num_hands=1) as hands:\n",
    "    path = os.path.join(IMAGE_PATH, \"normal\")\n",
    "    for dir in os.listdir(path)[:19]:\n",
    "        dir_path = os.path.join(path, dir)\n",
    "        for file in tqdm(os.listdir(dir_path)):\n",
    "            \n",
    "            image_path = os.path.join(dir_path, file)\n",
    "            image = cv2.imread(image_path)\n",
    "\n",
    "            if image is None:\n",
    "                continue\n",
    "\n",
    "            image, results = mediapipe_detection(image, hands)\n",
    "            \n",
    "            keypoints = extract_keypoints(image, results)\n",
    "            \n",
    "            if keypoints:\n",
    "                if not os.path.exists(os.path.join(DATA_PATH, dir)):\n",
    "                    os.makedirs(os.path.join(DATA_PATH, dir))\n",
    "                with open(os.path.join(DATA_PATH, dir, file.replace(\".jpg\", \".json\")), \"w\") as f:\n",
    "                    json.dump(keypoints, f)\n",
    "\n",
    "        print(\"Processed directory:\", dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603fcb66",
   "metadata": {},
   "source": [
    "**PREPROCESSING DATA FOR TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5805c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [15:34<00:00, 40.61s/it]\n"
     ]
    }
   ],
   "source": [
    "alphabets = np.array(os.listdir(DATA_PATH))\n",
    "label_map = {label: num for num, label in enumerate(alphabets)}\n",
    "\n",
    "data, labels = [], []\n",
    "  \n",
    "for label in tqdm(alphabets):\n",
    "    path = os.path.join(DATA_PATH, label)\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                keypoints = json.load(f)\n",
    "\n",
    "            flattened_keypoints = np.array([[landmrk[\"x\"], landmrk[\"y\"], landmrk[\"z\"]] for landmrk in keypoints]).flatten()\n",
    "\n",
    "            data.append(flattened_keypoints)\n",
    "            labels.append(label_map[label])\n",
    "\n",
    "        except (ValueError, TypeError, json.JSONDecodeError) as e:\n",
    "            print(f\"Skipping file {file_path} due to error: {e}\")\n",
    "\n",
    "# Convert to arrays and one-hot encode labels\n",
    "X = np.array(data)\n",
    "y = to_categorical(np.array(labels), num_classes=len(alphabets))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7ee43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "np.save(\"y_train.npy\", y_train)\n",
    "np.save(\"y_test.npy\", y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = np.load(\"X_train.npy\"), np.load(\"X_test.npy\"), np.load(\"y_train.npy\"), np.load(\"y_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ace35df",
   "metadata": {},
   "source": [
    "**TRAIN MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7448570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Sign Language Translation\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "log_dir = os.path.join(\"Logs\")\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model = Sequential()\n",
    "# The first convolutional layer\n",
    "model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# The second convolutional layer\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(23, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f6aac",
   "metadata": {},
   "source": [
    "**BUILD MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8137c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - categorical_accuracy: 0.5333 - loss: 1.5288\n",
      "Epoch 2/15\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - categorical_accuracy: 0.8148 - loss: 0.5808\n",
      "Epoch 3/15\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - categorical_accuracy: 0.8661 - loss: 0.4173\n",
      "Epoch 4/15\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - categorical_accuracy: 0.8954 - loss: 0.3276\n",
      "Epoch 5/15\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - categorical_accuracy: 0.9115 - loss: 0.2756\n",
      "Epoch 6/15\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - categorical_accuracy: 0.9203 - loss: 0.2541\n",
      "Epoch 7/15\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - categorical_accuracy: 0.9284 - loss: 0.2270\n",
      "Epoch 8/15\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - categorical_accuracy: 0.9343 - loss: 0.2102\n",
      "Epoch 9/15\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - categorical_accuracy: 0.9382 - loss: 0.1958\n",
      "Epoch 10/15\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - categorical_accuracy: 0.9427 - loss: 0.1831\n",
      "Epoch 11/15\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - categorical_accuracy: 0.9474 - loss: 0.1704\n",
      "Epoch 12/15\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - categorical_accuracy: 0.9508 - loss: 0.1634\n",
      "Epoch 13/15\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - categorical_accuracy: 0.9526 - loss: 0.1547\n",
      "Epoch 14/15\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - categorical_accuracy: 0.9529 - loss: 0.1481\n",
      "Epoch 15/15\n",
      "\u001b[1m3965/3965\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - categorical_accuracy: 0.9548 - loss: 0.1397\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1b2acc948d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=15, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e9dd8",
   "metadata": {},
   "source": [
    "**SAVE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7e5373c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"Models/action.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308d5c2",
   "metadata": {},
   "source": [
    "**LOAD MODEL AND TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a610b7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"Models/action.h5\")\n",
    "threshold = 0.8\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        image, results = mediapipe_detection(frame, hands)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            draw_landmarks(image, results)\n",
    "        \n",
    "            keypoints = extract_keypoints(image, results)\n",
    "            \n",
    "            if keypoints:\n",
    "                # Convert keypoints from list of dictionaries to flattened array\n",
    "                flattened_keypoints = np.array([[landmrk[\"x\"], landmrk[\"y\"], landmrk[\"z\"]] for landmrk in keypoints]).flatten()\n",
    "                \n",
    "                prediction = model.predict(np.expand_dims(flattened_keypoints, axis=0), verbose=0)\n",
    "                class_id = np.argmax(prediction)\n",
    "                confidence = np.max(prediction)\n",
    "\n",
    "                if confidence > threshold:\n",
    "                    label = alphabets[class_id]\n",
    "                    cv2.putText(image, f'{label} {int(confidence * 100)}%', (10, 70),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 0, 0), 3)\n",
    "\n",
    "        cv2.imshow('ASL Recognition', image)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
